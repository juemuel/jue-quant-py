import sys
import os

from pkg_resources import non_empty_lines
sys.path.append(os.path.join(os.path.dirname(__file__), '../../..'))

from app.services.data.data_service import get_stock_history
from app.services.storage.excel_storage_service import excel_storage
from app.services.analytics.indicator_service import IndicatorCalculator
from app.services.strategy.strategy_service import generate_ma_crossover_signal_from_indicators, generate_rsi_signal_from_indicators
from app.services.strategy.strategy_service import generate_unified_signals
from app.services.events.event_service import MarketEvent, EventType, EventSeverity
import datetime
from common.debug_utils import create_debug_logger, debug_strategy, debug_backtest, debug_data_provider, debug_event_provider
import pandas as pd
from core.logger import logger
# æ·»åŠ Excelå¯¼å‡ºåŠŸèƒ½
import openpyxl
# =========== å…¬ç”¨æ–¹æ³• ===========
def get_and_preprocess_stock_data(source="akshare", code="000001", market="SH", 
                                 start_date="20240101", end_date="20241201", page_size=1000, enable_logs=False):
    """
    è·å–å¹¶é¢„å¤„ç†è‚¡ç¥¨æ•°æ®çš„é€šç”¨æ–¹æ³•
    è¿”å›: (success: bool, df: DataFrame, message: str)
    """
    # 1. è·å–è‚¡ç¥¨å†å²æ•°æ®
    result = get_stock_history(
        source=source, code=code, market=market,
        start_date=start_date, end_date=end_date, page_size=page_size
    )
    
    if result.get('status') != 'success':
        return False, None, f"æ•°æ®è·å–å¤±è´¥: {result.get('message')}"
    
    # 2. æå–æ•°æ®å¹¶è½¬æ¢ä¸ºDataFrame
    data_info = result.get('data', {})
    data_list = data_info.get('list', [])
    
    if not data_list:
        return False, None, "æ²¡æœ‰è·å–åˆ°å®é™…æ•°æ®"
    
    df = pd.DataFrame(data_list)
    if(enable_logs == True):
        print("\n=== åŸå§‹æ•°æ®æ£€æŸ¥ ===")
        print(f"æ•°æ®æº: {source}, è‚¡ç¥¨ä»£ç : {code}, æ•°æ®è¡Œæ•°: {len(df)}")
        print(f"åˆ—å: {df.columns.tolist()}")
        if len(df) > 0:
            print("\nç¬¬ä¸€è¡ŒåŸå§‹æ•°æ®:")
            print(df.iloc[0].to_dict())
        print("=== åŸå§‹æ•°æ®æ£€æŸ¥ç»“æŸ ===\n")
    # 3. æ£€æŸ¥å¿…è¦åˆ—
    required_columns = ['æ”¶ç›˜ä»·', 'æ—¥æœŸ']
    missing_columns = [col for col in required_columns if col not in df.columns]
    if missing_columns:
        return False, None, f"ç¼ºå°‘å¿…è¦çš„åˆ—: {missing_columns}"
    
    # 4. æ•°æ®é¢„å¤„ç†
    df['æ—¥æœŸ'] = pd.to_datetime(df['æ—¥æœŸ'])
    df = df.sort_values('æ—¥æœŸ').reset_index(drop=True)
    df['æ”¶ç›˜ä»·'] = pd.to_numeric(df['æ”¶ç›˜ä»·'], errors='coerce')
    
    # åˆ é™¤æ— æ•ˆæ•°æ®
    nan_count = df['æ”¶ç›˜ä»·'].isna().sum()
    if nan_count > 0:
        df = df.dropna(subset=['æ”¶ç›˜ä»·'])
    
    # ä¸ºanalytic_serviceå‡†å¤‡æ•°æ®
    df['æ”¶ç›˜'] = df['æ”¶ç›˜ä»·']
    
    return True, df, f"æˆåŠŸè·å–å¹¶é¢„å¤„ç† {len(df)} è¡Œæ•°æ®"
def ensure_numeric_columns(df, columns):
    """
    ç¡®ä¿æŒ‡å®šåˆ—ä¸ºæ•°å€¼ç±»å‹çš„é€šç”¨æ–¹æ³•
    """
    df_copy = df.copy()
    for col in columns:
        if col in df_copy.columns:
            df_copy[col] = pd.to_numeric(df_copy[col], errors='coerce')
    return df_copy
def analyze_basic_signals(signal_df, signal_name="ä¿¡å·", display_columns=None):
    """
    åˆ†æå’Œæ˜¾ç¤ºä¿¡å·ç»Ÿè®¡çš„é€šç”¨æ–¹æ³•
    """
    if 'signal' not in signal_df.columns:
        return f"é”™è¯¯ï¼š{signal_name}æ•°æ®ä¸­æ²¡æœ‰signalåˆ—"
    
    # ç¡®ä¿signalåˆ—ä¸ºæ•°å€¼ç±»å‹
    df_copy = signal_df.copy()
    df_copy['signal'] = pd.to_numeric(df_copy['signal'], errors='coerce').fillna(0)
    
    # ç»Ÿè®¡ä¿¡å·
    buy_signals = df_copy[df_copy['signal'] == 1]
    sell_signals = df_copy[df_copy['signal'] == -1]
    non_zero_signals = df_copy[df_copy['signal'] != 0]

    result = f"\n=== {signal_name}ç»Ÿè®¡åˆ†æ ===\n"
    result += f"ä¹°å…¥ä¿¡å·æ•°é‡: {len(buy_signals)}\n"
    result += f"å–å‡ºä¿¡å·æ•°é‡: {len(sell_signals)}\n"
    
    if len(non_zero_signals) > 0:
        result += f"\næ‰€æœ‰{signal_name}éé›¶ä¿¡å· ({len(non_zero_signals)}ä¸ª)ï¼Œå‰10å¦‚ä¸‹ï¼š\n"
        if display_columns:
            available_cols = [col for col in display_columns if col in df_copy.columns]
            result += non_zero_signals[available_cols].head(10).to_string()
        else:
            result += non_zero_signals.head(10).to_string()
    else:
        result += f"\næ²¡æœ‰ç”Ÿæˆä»»ä½•{signal_name}"
    
    return result
def calculate_and_validate_indicator(df, calculator_method, indicator_name, **kwargs):
    """
    è®¡ç®—æŠ€æœ¯æŒ‡æ ‡å¹¶éªŒè¯ç»“æœçš„é€šç”¨æ–¹æ³•
    ä½¿ç”¨æ–°çš„ IndicatorCalculator
    """
    try:
        # åˆ›å»ºè®¡ç®—å™¨å®ä¾‹
        calculator = IndicatorCalculator()
        
        # è°ƒç”¨è®¡ç®—å™¨æ–¹æ³•
        result = calculator_method(calculator, df, **kwargs)
        
        if result.get('status') != 'success':
            return False, None, f"{indicator_name}è®¡ç®—å¤±è´¥: {result.get('message')}"
        
        data_list = result.get('data', [])
        if not data_list:
            return False, None, f"{indicator_name}è®¡ç®—è¿”å›ç©ºæ•°æ®"
        
        indicator_df = pd.DataFrame(data_list)
        return True, indicator_df, f"{indicator_name}è®¡ç®—æˆåŠŸ"
        
    except Exception as e:
        return False, None, f"{indicator_name}è®¡ç®—å¼‚å¸¸: {str(e)}"
def analyze_unified_signals(unified_result):
    """
    åˆ†æç»Ÿä¸€ä¿¡å·ç”Ÿæˆç»“æœ
    """
    if unified_result['status'] != 'success':
        return f"âŒ ç»Ÿä¸€ä¿¡å·ç”Ÿæˆå¤±è´¥: {unified_result.get('message')}"
    
    data = unified_result['data']
    unified_signals = data['unified_signals']
    
    result = "\n=== ç»Ÿä¸€ä¿¡å·ç”Ÿæˆåˆ†æ ===\n"
    result += f"âœ“ æ•°æ®é©±åŠ¨ä¿¡å·æ•°é‡: {data['data_signals_count']}\n"
    result += f"âœ“ äº‹ä»¶é©±åŠ¨ä¿¡å·æ•°é‡: {data['event_signals_count']}\n"
    result += f"âœ“ ç»Ÿä¸€åæ€»ä¿¡å·æ•°é‡: {data['total_signals']}\n"
    
    # åˆ†æä¿¡å·ç±»å‹åˆ†å¸ƒ
    if unified_signals:
        signal_types = {}
        signal_strengths = []
        
        for signal in unified_signals:
            signal_type = signal.get('type', 'unknown')
            signal_types[signal_type] = signal_types.get(signal_type, 0) + 1
            
            strength = signal.get('strength', 0)
            if strength > 0:
                signal_strengths.append(strength)
        
        result += "\nç»Ÿä¸€åçš„ä¿¡å·ç±»å‹åˆ†å¸ƒ:\n"
        for sig_type, count in signal_types.items():
            result += f"  - {sig_type}: {count}ä¸ª\n"
        
        if signal_strengths:
            avg_strength = sum(signal_strengths) / len(signal_strengths)
            result += f"\nå¹³å‡ä¿¡å·å¼ºåº¦: {avg_strength:.3f}\n"
            result += f"æœ€å¼ºä¿¡å·å¼ºåº¦: {max(signal_strengths):.3f}\n"
            result += f"æœ€å¼±ä¿¡å·å¼ºåº¦: {min(signal_strengths):.3f}\n"
        
        # æ˜¾ç¤ºå‰5ä¸ªä¿¡å·è¯¦æƒ…
        result += "\nå‰10ä¸ªç»Ÿä¸€ä¿¡å·è¯¦æƒ…:\n"
        for i, signal in enumerate(unified_signals[:10]):
            signal_type = signal.get('signal_type', 'unknown')
            direction = signal.get('direction', 0)
            reason = signal.get('reason', signal.get('metadata', {}).get('reason', 'N/A'))
            result += f"  {i+1}. ç±»å‹: {signal_type}({direction}), "
            result += f"å¼ºåº¦: {signal.get('strength', 0):.3f}, "
            result += f"æ—¶é—´: {signal.get('timestamp', 'N/A')}\n"
            result += f"     åŸå› : {reason}\n"
    
    return result
def create_mock_events_data(df=None, event_count=20):
    """
    åˆ›å»ºæ¨¡æ‹Ÿäº‹ä»¶æ•°æ®ç”¨äºæµ‹è¯•
    
    Args:
        df: DataFrameï¼ŒåŒ…å«è‚¡ç¥¨æ•°æ®ï¼Œç”¨äºç¡®å®šæ—¶é—´èŒƒå›´
        event_count: è¦ç”Ÿæˆçš„äº‹ä»¶æ•°é‡
        
    Returns:
        dict: {
            'success': bool,
            'data': list[MarketEvent] | None,
            'message': str
        }
    """
    import random
    
    try:
        # ç¡®å®šæ—¶é—´èŒƒå›´
        if df is None or df.empty:
            # å¦‚æœæ²¡æœ‰æä¾›DataFrameï¼Œä½¿ç”¨é»˜è®¤æ—¥æœŸèŒƒå›´
            start_dt = datetime.datetime(2024, 1, 1)
            end_dt = datetime.datetime(2024, 12, 1)
            debug_event_provider("ä½¿ç”¨é»˜è®¤æ—¶é—´èŒƒå›´", {
                'start_date': start_dt.strftime('%Y-%m-%d'),
                'end_date': end_dt.strftime('%Y-%m-%d'),
                'reason': 'DataFrameä¸ºç©ºæˆ–æœªæä¾›'
            })
        else:
            # ä»DataFrameä¸­æå–æ—¥æœŸèŒƒå›´
            if 'æ—¥æœŸ' in df.columns:
                dates = pd.to_datetime(df['æ—¥æœŸ'])
                start_dt = dates.min().to_pydatetime()
                end_dt = dates.max().to_pydatetime()
                debug_event_provider(f"ä»è‚¡ç¥¨æ•°æ®æå–æ—¶é—´èŒƒå›´: {start_dt.strftime('%Y-%m-%d')} ~ {end_dt.strftime('%Y-%m-%d')}", {
                    'start_date': start_dt.strftime('%Y-%m-%d'),
                    'end_date': end_dt.strftime('%Y-%m-%d'),
                    'data_rows': len(df)
                })
            else:
                # å¦‚æœæ²¡æœ‰æ—¥æœŸåˆ—ï¼Œä½¿ç”¨é»˜è®¤èŒƒå›´
                start_dt = datetime.datetime(2024, 1, 1)
                end_dt = datetime.datetime(2024, 12, 1)
                debug_event_provider("DataFrameä¸­æ— æ—¥æœŸåˆ—ï¼Œä½¿ç”¨é»˜è®¤æ—¶é—´èŒƒå›´", {
                    'start_date': start_dt.strftime('%Y-%m-%d'),
                    'end_date': end_dt.strftime('%Y-%m-%d'),
                    'available_columns': list(df.columns) if df is not None else []
                })
        
        # è®¡ç®—æ—¥æœŸèŒƒå›´
        date_range = (end_dt - start_dt).days
        if date_range <= 0:
            date_range = 365  # é»˜è®¤ä¸€å¹´èŒƒå›´
            debug_event_provider("æ—¥æœŸèŒƒå›´æ— æ•ˆï¼Œä½¿ç”¨é»˜è®¤365å¤©", {
                'original_range': (end_dt - start_dt).days,
                'adjusted_range': date_range
            })
        
        # éªŒè¯äº‹ä»¶æ•°é‡
        if event_count <= 0:
            error_msg = f"äº‹ä»¶æ•°é‡å¿…é¡»å¤§äº0ï¼Œå½“å‰å€¼: {event_count}"
            debug_event_provider(error_msg, {'event_count': event_count}, "ERROR")
            return {
                'success': False,
                'data': None,
                'message': error_msg
            }
        
        debug_event_provider(f"å¼€å§‹ç”Ÿæˆäº‹ä»¶ï¼Œå‚æ•°ç¡®è®¤", {
            'event_count': event_count,
            'date_range_days': date_range,
            'start_date': start_dt.strftime('%Y-%m-%d'),
            'end_date': end_dt.strftime('%Y-%m-%d')
        })
        
        # äº‹ä»¶æ¨¡æ¿å®šä¹‰
        event_templates = [
            {
                "type": EventType.NEWS,
                "titles": ["é‡å¤§åˆä½œåè®®ç­¾ç½²", "æ–°äº§å“å‘å¸ƒä¼šæˆåŠŸä¸¾åŠ", "ä¸šç»©å¤§å¹…å¢é•¿", "è·å¾—é‡è¦å¥–é¡¹"],
                "sentiment_range": (0.6, 0.9),  # æ­£é¢æƒ…æ„ŸèŒƒå›´
                "severity": EventSeverity.HIGH
            },
            {
                "type": EventType.NEWS,
                "titles": ["ç›‘ç®¡è°ƒæŸ¥å¯åŠ¨", "é‡å¤§è¿è§„äº‹ä»¶", "ä¸šç»©å¤§å¹…ä¸‹æ»‘", "é«˜ç®¡ç¦»èŒé£æ³¢"],
                "sentiment_range": (-0.9, -0.6),  # è´Ÿé¢æƒ…æ„ŸèŒƒå›´
                "severity": EventSeverity.HIGH
            },
            {
                "type": EventType.EARNINGS,
                "titles": ["å³å°†å‘å¸ƒå­£åº¦è´¢æŠ¥", "å¹´åº¦ä¸šç»©é¢„å‘Š", "ä¸­æœŸä¸šç»©è¯´æ˜ä¼š", "æŠ•èµ„è€…å…³ç³»æ´»åŠ¨"],
                "sentiment_range": (-0.3, 0.7),   # ä¸­æ€§åˆ°æ­£é¢æƒ…æ„ŸèŒƒå›´
                "severity": EventSeverity.MEDIUM
            }
        ]
        
        debug_event_provider(f"ä½¿ç”¨ {len(event_templates)} ä¸ªäº‹ä»¶æ¨¡æ¿ç”Ÿæˆæ•°æ®")
        # åˆå§‹åŒ–äº‹ä»¶åˆ—è¡¨
        mock_events = []
        # ç”ŸæˆæŒ‡å®šæ•°é‡çš„äº‹ä»¶
        for i in range(event_count):
            # éšæœºé€‰æ‹©äº‹ä»¶æ¨¡æ¿
            template = random.choice(event_templates)
            
            # éšæœºç”Ÿæˆæ—¥æœŸ
            random_days = random.randint(0, date_range)
            event_date = start_dt + datetime.timedelta(days=random_days)
            
            # éšæœºé€‰æ‹©æ ‡é¢˜å’Œæƒ…æ„Ÿåˆ†æ•°
            title = random.choice(template["titles"])
            sentiment_min, sentiment_max = template["sentiment_range"]
            sentiment_score = random.uniform(sentiment_min, sentiment_max)
            
            # åˆ›å»ºäº‹ä»¶
            event = MarketEvent(
                event_id=f"event_{i+1:03d}",
                event_type=template["type"],
                symbol="000001",
                timestamp=event_date,
                title=title,
                content=f"{title}çš„è¯¦ç»†å†…å®¹æè¿°",
                severity=template["severity"],
                sentiment_score=sentiment_score,
                keywords=title.split(),
                source="æ¨¡æ‹Ÿæ•°æ®æº",
                metadata={"event_index": i+1, "template_type": template["type"].value}
            )
            
            mock_events.append(event)
        
        # æŒ‰æ—¶é—´æ’åº
        mock_events.sort(key=lambda x: x.timestamp)
        
        # ç»Ÿè®¡ç”Ÿæˆçš„äº‹ä»¶
        event_types = {}
        sentiment_stats = {'positive': 0, 'negative': 0, 'neutral': 0}
        severity_stats = {}
        
        for event in mock_events:
            # ç»Ÿè®¡äº‹ä»¶ç±»å‹
            event_type = event.event_type.value
            event_types[event_type] = event_types.get(event_type, 0) + 1
            
            # ç»Ÿè®¡æƒ…æ„Ÿåˆ†å¸ƒ
            if event.sentiment_score > 0.2:
                sentiment_stats['positive'] += 1
            elif event.sentiment_score < -0.2:
                sentiment_stats['negative'] += 1
            else:
                sentiment_stats['neutral'] += 1
            
            # ç»Ÿè®¡ä¸¥é‡ç¨‹åº¦
            severity = event.severity.value
            severity_stats[severity] = severity_stats.get(severity, 0) + 1
        
        debug_event_provider(f"æˆåŠŸç”Ÿæˆ {len(mock_events)} ä¸ªäº‹ä»¶", {
            'event_count': len(mock_events),
            'time_range': {
                'start': mock_events[0].timestamp.strftime('%Y-%m-%d'),
                'end': mock_events[-1].timestamp.strftime('%Y-%m-%d')
            },
            'statistics': {
                'event_types': event_types,
                'sentiment_distribution': sentiment_stats,
                'severity_distribution': severity_stats
            }
        })
        
        # æ˜¾ç¤ºå‰å‡ ä¸ªäº‹ä»¶çš„è¯¦ç»†ä¿¡æ¯
        debug_event_provider("å‰5ä¸ªäº‹ä»¶è¯¦æƒ…:")
        for i, event in enumerate(mock_events[:5]):
            debug_event_provider(f"äº‹ä»¶ {i+1}: {event.title}", {
                'event_id': event.event_id,
                'type': event.event_type.value,
                'sentiment': round(event.sentiment_score, 3),
                'severity': event.severity.value,
                'timestamp': event.timestamp.strftime('%Y-%m-%d %H:%M:%S')
            })
        
        return {
            'success': True,
            'data': mock_events,
            'message': f"æˆåŠŸç”Ÿæˆ {len(mock_events)} ä¸ªæ¨¡æ‹Ÿäº‹ä»¶ï¼Œæ—¶é—´èŒƒå›´: {mock_events[0].timestamp.strftime('%Y-%m-%d')} åˆ° {mock_events[-1].timestamp.strftime('%Y-%m-%d')}"
        }
        
    except Exception as e:
        error_msg = f"ç”Ÿæˆæ¨¡æ‹Ÿäº‹ä»¶æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"
        debug_event_provider(error_msg, level="ERROR")
        return {
            'success': False,
            'data': None,
            'message': error_msg
        }
# ============ åŸºç¡€å•ä¿¡å·ç”Ÿæˆ(ä½¿ç”¨strategy_serviceä¸­çš„å•ä¸€ä¿¡å·çš„ç”Ÿæˆå‡½æ•°) ============
# MAä½¿ç”¨ç®€å•çš„é‡‘å‰æ­»å‰åˆ¤æ–­,å›ºå®šä½¿ç”¨MA5å’ŒMA20
# RSIå›ºå®šé˜ˆå€¼è¶…å–30,è¶…ä¹°70,è§¦åŠå°±ç”Ÿæˆ
def debug_basic_strategy_flow():

    """
    ä½¿ç”¨å°è£…æ–¹æ³•çš„ç®€åŒ–ç‰ˆåŸºç¡€ç­–ç•¥æµç¨‹è°ƒè¯•
    """
    print("=== å¼€å§‹è°ƒè¯•åŸºç¡€ç­–ç•¥æµç¨‹ï¼ˆç®€åŒ–ç‰ˆï¼‰===")
    
    try:
        # 1. è·å–å’Œé¢„å¤„ç†æ•°æ®
        success, df, message = get_and_preprocess_stock_data()
        if not success:
            print(f"æ•°æ®å‡†å¤‡å¤±è´¥: {message}")
            return
        print(f"âœ“ {message}")
        
        # 2. è®¡ç®—ç§»åŠ¨å¹³å‡çº¿
        success, df_with_ma, message = calculate_and_validate_indicator(
            df, IndicatorCalculator.calculate_moving_averages, "ç§»åŠ¨å¹³å‡çº¿", periods=[5, 20]
        )
        if not success:
            print(f"âœ— {message}")
            return
        print(f"âœ“ {message}")
        
        # ç¡®ä¿æ•°å€¼ç±»å‹
        # ç¡®ä¿æ•°å€¼ç±»å‹ - æ³¨æ„æ–°çš„åˆ—åæ ¼å¼æ˜¯ SMA5, SMA20
        df_with_ma = ensure_numeric_columns(df_with_ma, ['æ”¶ç›˜ä»·', 'æ”¶ç›˜', 'SMA5', 'SMA20'])
        
        # é‡å‘½ååˆ—ä»¥ä¿æŒå…¼å®¹æ€§
        if 'SMA5' in df_with_ma.columns:
            df_with_ma['MA5'] = df_with_ma['SMA5']
        if 'SMA20' in df_with_ma.columns:
            df_with_ma['MA20'] = df_with_ma['SMA20']
            
        # 3. è®¡ç®—RSI
        success, df_with_rsi, message = calculate_and_validate_indicator(
            df, IndicatorCalculator.calculate_rsi, "RSIæŒ‡æ ‡", period=14
        )
        if not success:
            print(f"âœ— {message}")
            df_with_rsi = df_with_ma  # ä½¿ç”¨MAæ•°æ®ç»§ç»­
        else:
            print(f"âœ“ {message}")
            df_with_rsi = ensure_numeric_columns(df_with_rsi, ['æ”¶ç›˜ä»·', 'æ”¶ç›˜', 'RSI'])
        
        # åˆå§‹åŒ–ä¿¡å·å˜é‡
        ma_signals_row = []
        rsi_signals_row = []
        # 4. ç”ŸæˆMAäº¤å‰ä¿¡å·
        # åœ¨ç”ŸæˆMAä¿¡å·å‰æ·»åŠ è°ƒè¯•ä¿¡æ¯
        logger.debug(f"[Service]df_with_maåˆ—: {df_with_ma.columns.tolist()}")
        logger.info(f"[DebugBasicStrategy]å¼€å§‹ç”ŸæˆMAäº¤å‰ä¿¡å·ï¼Œæ•°æ®è¡Œæ•°: {len(df_with_ma)}")
        logger.info(f"[DebugBasicStrategy]MA5é•¿åº¦: {len(df_with_ma['MA5']) if 'MA5' in df_with_ma.columns else 0}, MA20é•¿åº¦: {len(df_with_ma['MA20']) if 'MA20' in df_with_ma.columns else 0}")
        ma_signal_result = generate_ma_crossover_signal_from_indicators(df_with_ma, short_period=5, long_period=20)
        if ma_signal_result['status'] == 'success':
            ma_signals_row = ma_signal_result['data']
            signal_df = pd.DataFrame(ma_signals_row)
            print(analyze_basic_signals(signal_df, "MAäº¤å‰ä¿¡å·", ['æ—¥æœŸ', 'MA5', 'MA20', 'signal']))
        logger.info(f"[DebugBasicStrategy]MAäº¤å‰ä¿¡å·æ•°é‡ï¼ˆå«é›¶ï¼‰: {len(ma_signals_row)}")
        # 5. ç”ŸæˆRSIä¿¡å·
        # åœ¨ç”ŸæˆRSIä¿¡å·å‰æ·»åŠ è°ƒè¯•ä¿¡æ¯
        logger.info(f"[DebugBasicStrategy]df_with_rsiåˆ—: {df_with_rsi.columns.tolist()}")
        logger.info(f"[DebugBasicStrategy]å¼€å§‹ç”ŸæˆRSIä¿¡å·ï¼ŒRSIé•¿åº¦: {len(df_with_rsi['RSI14']) if 'RSI14' in df_with_rsi.columns else 0}")
        rsi_signal_result = generate_rsi_signal_from_indicators(df_with_rsi, period=14, oversold=30, overbought=70)
        if rsi_signal_result['status'] == 'success':
            rsi_signals_row = rsi_signal_result['data']
            signal_df = pd.DataFrame(rsi_signals_row)
            print(analyze_basic_signals(signal_df, "RSIä¿¡å·", ['æ—¥æœŸ', 'æ”¶ç›˜ä»·', 'RSI', 'signal']))
        # å¯¼å‡ºExcelæ–‡ä»¶ - åŸºç¡€ç­–ç•¥æµç¨‹
        try:
            excel_file = excel_storage.save_basic_strategy_data(
                ma_signals=ma_signals_row,
                rsi_signals=rsi_signals_row,
                raw_data=df_with_rsi,
                filename_prefix="basic_strategy_signals"
            )
            print(f"\nğŸ“Š åŸºç¡€ç­–ç•¥ä¿¡å·å·²ä¿å­˜åˆ°: {excel_file}")
        except Exception as e:
                logger.error(f"å¯¼å‡ºExcelæ–‡ä»¶å¤±è´¥: {e}")
                print(f"âŒ Excelå¯¼å‡ºå¤±è´¥: {e}")
        print("\n=== åŸºç¡€ç­–ç•¥æµç¨‹è°ƒè¯•å®Œæˆ ===")
        # ç»Ÿè®¡éé›¶ä¿¡å· - ä¿®æ­£è¿‡æ»¤é€»è¾‘
        def is_nonzero_signal(signal_dict):
            signal_value = signal_dict.get('signal', 0)
            # å¤„ç†å„ç§å¯èƒ½çš„é›¶å€¼è¡¨ç¤º
            if signal_value is None:
                return False
            if isinstance(signal_value, str):
                try:
                    signal_value = float(signal_value)
                except (ValueError, TypeError):
                    return False
            return signal_value != 0 and signal_value != 0.0
        
        ma_nonzero = [s for s in ma_signals_row if is_nonzero_signal(s)]
        rsi_nonzero = [s for s in rsi_signals_row if is_nonzero_signal(s)]
        
        logger.info(f"[DebugBasicStrategy]MAéé›¶ä¿¡å·æ•°é‡: {len(ma_nonzero)}")
        logger.info(f"[DebugBasicStrategy]RSIéé›¶ä¿¡å·æ•°é‡: {len(rsi_nonzero)}")
        logger.info(f"[DebugBasicStrategy]æ€»éé›¶ä¿¡å·æ•°é‡: {len(ma_nonzero) + len(rsi_nonzero)}")
        
    except Exception as e:
        logger.error(f"è°ƒè¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        print(f"\né”™è¯¯: {e}")

# ============ ç»Ÿä¸€ä¿¡å·ç”Ÿæˆ ============
# æ•°æ®é©±åŠ¨ä¸­ä½¿ç”¨MAå’ŒRSI,äº‹ä»¶é©±åŠ¨ä¸­ä½¿ç”¨æ–°é—»æƒ…æ„Ÿå’Œè´¢æŠ¥å‘å¸ƒ
# é»˜è®¤MAä¿¡å·ç”Ÿæˆ:MA5/MA20;
# é»˜è®¤RSIä¿¡å·ç”Ÿæˆ:å‘¨æœŸ14,è¶…ä¹°è¶…å–30/70; å¹¶ä¸”éœ€è¦æ»¡è¶³æˆäº¤é‡volume > avg_volume * 1.2
def debug_unified_signals():
    """
    è°ƒè¯•ç»Ÿä¸€ä¿¡å·ç”Ÿæˆå™¨
    """
    # åˆ›å»ºè°ƒè¯•æ—¥å¿—ç®¡ç†å™¨
    logger = create_debug_logger("è°ƒè¯•ç»Ÿä¸€ä¿¡å·ç”ŸæˆåŠŸèƒ½", "strategy")
    logger.start_session("è°ƒè¯•ç»Ÿä¸€ä¿¡å·ç”ŸæˆåŠŸèƒ½", "æµ‹è¯•æ•°æ®ä¿¡å·å’Œäº‹ä»¶ä¿¡å·ç”Ÿæˆ")
    
    try:
        # 1. æ•°æ®è·å–
        logger.step_start("1. æ•°æ®è·å–", "è·å–å’Œé¢„å¤„ç†è‚¡ç¥¨æ•°æ®")
        success, df, message = get_and_preprocess_stock_data()
        if not success:
            logger.step_error("æ•°æ®è·å–å¤±è´¥", message)
            return
        logger.step_success("æ•°æ®è·å–", f"è·å– {len(df)} è¡Œæ•°æ®", {
            'data_rows': len(df),
            'date_range': f"{df['æ—¥æœŸ'].min()} ~ {df['æ—¥æœŸ'].max()}"
        })
        # 2. äº‹ä»¶è·å–
        logger.step_start("2. äº‹ä»¶è·å–", "åˆ›å»ºæ¨¡æ‹Ÿäº‹ä»¶æ•°æ®")
        events_result = create_mock_events_data(
            df,
            event_count=300
        )
        # æ£€æŸ¥äº‹ä»¶ç”Ÿæˆç»“æœ
        if not events_result['success']:
            logger.step_error("äº‹ä»¶è·å–", events_result['message'])
            return
        events_data = events_result['data']
        logger.step_success("äº‹ä»¶è·å–", events_result['message'], {
            'events_generated': len(events_data),
            'time_range': {
                'start': events_data[0].timestamp.strftime('%Y-%m-%d'),
                'end': events_data[-1].timestamp.strftime('%Y-%m-%d')
            }
        })
        
        # 3. é…ç½®ä¿¡å·ç”Ÿæˆå‚æ•°
        logger.step_start("3. ä¿¡å·é…ç½®", "åˆ›å»ºæ•°æ®ä¿¡å·å’Œäº‹ä»¶ä¿¡å·é…ç½®")
        
        # æ•°æ®é©±åŠ¨ä¿¡å·è§„åˆ™é…ç½®
        data_signal_config = {
            'ma_crossover': {
                'enable': True,
                'use_parameterized': True,   # Falseå›ºå®šå‚æ•°ï¼Œä¸debug_basic_strategy_flowä¸€è‡´ï¼›Trueå‚æ•°åŒ–,æ”¯æŒadaptiveè‡ªé€‚åº”ï¼Œfilterè¿‡æ»¤å‚æ•°é…ç½®ï¼›
                'short_period': 5,           # ä¸debug_basic_strategy_flowç›¸åŒ
                'long_period': 20,           # ä¸debug_basic_strategy_flowç›¸åŒ
                'adaptive': True,            # å¯ç”¨è‡ªé€‚åº”å‘¨æœŸé€‚é…ï¼ˆéœ€è¦use_parameterizedä¸ºTrueï¼‰
                'filter_config': {
                    'volatility_filter': {'enable': True, 'min_volatility': 0.3, 'max_volatility': 1},  # ç¦ç”¨è¿‡æ»¤å™¨
                    'volume_confirmation': {'enable': False, 'volume_multiplier': 1, 'lookback_days': 20},
                    'trend_strength_filter': {'enable': False, 'min_adx': 25},
                    'signal_strength_filter': {'enable': False, 'min_strength': 0.3}
                }
            },
            'rsi': {
                'enable': True,
                'use_parameterized': True,   # Falseå›ºå®šå‚æ•°ï¼Œä¸debug_basic_strategy_flowä¸€è‡´ï¼›Trueå‚æ•°åŒ–,æ”¯æŒadaptiveè‡ªé€‚åº”ï¼Œfilterè¿‡æ»¤å‚æ•°é…ç½®ï¼›
                'period': 14,                # ä¸debug_basic_strategy_flowç›¸åŒ
                'oversold': 30,              # ä¸debug_basic_strategy_flowç›¸åŒ
                'overbought': 70,            # ä¸debug_basic_strategy_flowç›¸åŒ
                'adaptive': True,            # å¯ç”¨è‡ªé€‚åº”å‘¨æœŸé€‚é…ï¼ˆéœ€è¦use_parameterizedä¸ºTrueï¼‰
                'filter_config': {
                    'volume_confirmation': {'enable': True, 'volume_multiplier': 1, 'lookback_days': 20},  # ç¦ç”¨è¿‡æ»¤å™¨
                    'volatility_filter': {'enable': False, 'min_volatility': 0.3, 'max_volatility': 0.5},
                    'trend_strength_filter': {'enable': False, 'min_adx': 25},
                    'signal_strength_filter': {'enable': False, 'min_strength': 0.3}
                }
            }
        }
        # äº‹ä»¶é©±åŠ¨ä¿¡å·è§„åˆ™é…ç½®
        event_signal_config = {
            'news_sentiment': {
                'enable': True,
                'use_parameterized': True,  # ä½¿ç”¨å‚æ•°åŒ–ç‰ˆæœ¬
                'sentiment_threshold': 0.8,  # è‡ªå®šä¹‰é˜ˆå€¼
                'severity_levels': [EventSeverity.HIGH, EventSeverity.CRITICAL]
            },
            'earnings': {
                'enable': True,
                'use_parameterized': False  # ä½¿ç”¨å›ºå®šå‚æ•°ç‰ˆæœ¬
            },
            'keyword_trigger': {
                'enable': True,
                'use_parameterized': True,  # ä½¿ç”¨å‚æ•°åŒ–ç‰ˆæœ¬
                'positive_keywords': ['çªç ´', 'åˆ›æ–°é«˜', 'åˆ©å¥½'],
                'negative_keywords': ['æš´è·Œ', 'äºæŸ', 'é£é™©'],
                'strength': 0.7,
            }
        }
         # é…ç½®å®Œæˆï¼Œè®°å½•è¯¦ç»†ä¿¡æ¯
        config_details = {
            'data_signals_count': len([k for k, v in data_signal_config.items() if v.get('enable', False)]),
            'event_signals_count': len([k for k, v in event_signal_config.items() if v.get('enable', False)]),
            'ma_config': f"MAäº¤å‰({data_signal_config['ma_crossover']['short_period']},{data_signal_config['ma_crossover']['long_period']})",
            'rsi_config': f"RSI({data_signal_config['rsi']['period']})",
            'news_threshold': event_signal_config.get('news_sentiment', {}).get('sentiment_threshold', 'é»˜è®¤'),
            'earnings_mode': "å‚æ•°åŒ–" if event_signal_config.get('earnings', {}).get('use_parameterized', False) else "å›ºå®šå‚æ•°",
            'keyword_strength': event_signal_config.get('keyword_trigger', {}).get('strength', 'é»˜è®¤')
        }
        
        logger.step_success("ä¿¡å·é…ç½®", "ä¿¡å·é…ç½®åˆ›å»ºå®Œæˆ", config_details)

        # 4. ç”Ÿæˆç»Ÿä¸€ä¿¡å·
        logger.step_start("4. ä¿¡å·ç”Ÿæˆ", "ç”Ÿæˆç»Ÿä¸€ä¿¡å·")
        # 4.1 ç”Ÿæˆç»Ÿä¸€ç»„åˆä¿¡å·
        # unified_result = generate_unified_signals(
        #     price_data=df,
        #     events_data=events_data,
        #     data_signal_config=data_signal_config,
        #     event_signal_config=event_signal_config
        # )
        # print(analyze_unified_signals(unified_result))
        # 4.2 ç”Ÿæˆä»…æ•°æ®é©±åŠ¨çš„ä¿¡å·
        data_only_result = generate_unified_signals(
            price_data=df,
            events_data=None,  # ä¸æä¾›äº‹ä»¶æ•°æ®
            data_signal_config=data_signal_config,
            event_signal_config=None
        )
        print(analyze_unified_signals(data_only_result))
        # 4.3 ç”Ÿæˆä»…äº‹ä»¶é©±åŠ¨çš„ä¿¡å·
        # event_only_result = generate_unified_signals(
        #     price_data=df,
        #     events_data=events_data,
        #     data_signal_config=None,
        #     event_signal_config=event_signal_config
        # )
        # print(analyze_unified_signals(event_only_result))
        # 4.4 ç”Ÿæˆé»˜è®¤é…ç½®çš„ä¿¡å·
        # default_result = generate_unified_signals(price_data=df, events_data=events_data)
        # print(analyze_unified_signals(default_result))
        
        # 5. åˆ†æç»“æœ
        # 5. å¯¼å‡ºåˆ°Excelæ–‡ä»¶ï¼ˆæ›¿æ¢åŸæœ‰çš„Excelå¯¼å‡ºä»£ç ï¼‰
        print("\n5. å¯¼å‡ºæ•°æ®åˆ°Excel...")
        
        try:
            # æå–ä¿¡å·æ•°æ®
            data_signals = data_only_result.get('data', {}).get('data_signals')
            event_signals = data_only_result.get('data', {}).get('event_signals')
            unified_signals = data_only_result.get('data', {}).get('unified_signals')
            
            # åˆ›å»ºæ±‡æ€»ä¿¡æ¯
            summary_info = {
                'é¡¹ç›®': ['è‚¡ç¥¨ä»£ç ', 'æ•°æ®æ—¶é—´èŒƒå›´', 'è‚¡ç¥¨æ•°æ®è¡Œæ•°', 'æ¨¡æ‹Ÿäº‹ä»¶æ•°é‡', 
                        'æ•°æ®ä¿¡å·æ•°é‡', 'äº‹ä»¶ä¿¡å·æ•°é‡', 'ç»Ÿä¸€ä¿¡å·æ•°é‡', 'å¯¼å‡ºæ—¶é—´'],
                'å€¼': [
                    '000001.SH',
                    f"{df['æ—¥æœŸ'].min()} ~ {df['æ—¥æœŸ'].max()}",
                    len(df),
                    len(events_data),
                    data_only_result.get('data', {}).get('data_signals_count', 0),
                    data_only_result.get('data', {}).get('event_signals_count', 0),
                    data_only_result.get('data', {}).get('total_signals', 0),
                    datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                ]
            }
            
            excel_file = excel_storage.save_unified_strategy_data(
                stock_data=df,
                data_signals=data_signals,
                event_signals=event_signals,
                unified_signals=unified_signals,
                events_data=events_data,
                summary_info=summary_info,
                filename_prefix="unified_signals_debug"
            )
            
            print(f"âœ“ Excelæ–‡ä»¶å·²ä¿å­˜: {excel_file}")
            print(f"  åŒ…å«å·¥ä½œè¡¨: è‚¡ç¥¨å†å²æ•°æ®, æ•°æ®ä¿¡å·, äº‹ä»¶ä¿¡å·, ç»Ÿä¸€ä¿¡å·, æ±‡æ€»ä¿¡æ¯")
            
        except Exception as e:
            print(f"âŒ Excelå¯¼å‡ºå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
        logger.end_session("è°ƒè¯•å®Œæˆ")
    except Exception as e:
        logger.error(f"ç»Ÿä¸€ä¿¡å·è°ƒè¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        print(f"\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    debug_unified_signals()
    # debug_basic_strategy_flow()